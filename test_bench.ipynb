{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_scores(splits: dict, corpus: list[str])->dict:\n",
    "    subword_freq = defaultdict(int)\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        n = len(splits[word])\n",
    "        for i, c in enumerate(splits[word]):\n",
    "            subword_freq[c] += 1\n",
    "            if i + 1 < n:\n",
    "                pair_freq[(c, splits[word][i + 1])] += 1\n",
    "    # scores = { (p0, p1): p_f / (subword_freq[p0] * subword_freq[p1]) for (p0, p1), p_f in pair_freq.items() }\n",
    "    # scores = { (p0, p1): p_f / (subword_freq[p0]) for (p0, p1), p_f in pair_freq.items() }\n",
    "    scores = { (p0, p1): p_f for (p0, p1), p_f in pair_freq.items() }\n",
    "    return scores\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.vocab = []\n",
    "        self.unk_token = '<unk>'\n",
    "\n",
    "    def load(self, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "    def train(self, corpus: list[str], vocab_size: int, unk_token: str = '<unk>'):\n",
    "        \"\"\"\n",
    "        corpus: a list of words from pre-tokenization\n",
    "        \"\"\"\n",
    "        self.vocab = [unk_token]\n",
    "        vocab_set = {unk_token}\n",
    "        splits = {}\n",
    "        words = set()\n",
    "        for word in corpus:\n",
    "            split = []\n",
    "            for id, c in enumerate(word):\n",
    "                if id != 0:\n",
    "                    c = '##' + c\n",
    "                if c not in vocab_set:\n",
    "                    self.vocab.append(c)\n",
    "                    vocab_set.add(c)\n",
    "                split.append(c)\n",
    "            splits[word] = split\n",
    "            words.add(word)\n",
    "\n",
    "        # print(splits)\n",
    "\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            \n",
    "            if len(self.vocab) % 100 == 0:\n",
    "                print(best_p, best_s)\n",
    "                print(len(self.vocab))\n",
    "\n",
    "            scores = get_scores(splits, corpus)\n",
    "            if len(scores) == 0:\n",
    "                break\n",
    "\n",
    "            best_p, best_s = '', None\n",
    "            for p, s in scores.items():\n",
    "                if best_s is None or s > best_s:\n",
    "                    best_p = p\n",
    "                    best_s = s\n",
    "            if best_s == 1:\n",
    "                break\n",
    "            new_subword = best_p[0] + best_p[1][2:]\n",
    "            self.vocab.append(new_subword)\n",
    "            \n",
    "            for word in words:\n",
    "                split = splits[word]\n",
    "                new_split = []\n",
    "                ignore = -1\n",
    "                for i in range(len(split)):\n",
    "                    if i == ignore:\n",
    "                        continue\n",
    "                    sw = split[i]\n",
    "                    if i + 1 < len(split) and (sw + split[i + 1][2:] == new_subword):\n",
    "                        new_split.append(new_subword)\n",
    "                        ignore = i + 1\n",
    "                    else:\n",
    "                        new_split.append(sw)\n",
    "                splits[word] = new_split       \n",
    "\n",
    "            # break             \n",
    "\n",
    "    def tokenize(self, word: str)->list[str]:\n",
    "        \"\"\"\n",
    "        word: a word produced from pre-tokenization\n",
    "        \"\"\"\n",
    "        toks = []\n",
    "        first = True\n",
    "        while len(word):\n",
    "            tok = None\n",
    "            pos = None\n",
    "            for i in range(len(word)):\n",
    "                prefix = '##' + word[:i + 1] if not first else word[:i + 1]\n",
    "                if prefix in self.vocab:\n",
    "                    tok = prefix\n",
    "                    pos = i + 1\n",
    "            if tok is None:\n",
    "                return [self.unk_token]\n",
    "            word = word[pos:]\n",
    "            toks.append(tok)\n",
    "            first = False\n",
    "        return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##n', '##g', '##e', '##l']\n",
      "['o', '##f']\n",
      "['v', '##it', '##a', '##l', '##it', '##y']\n",
      "['c', '##e', '##m', '##e', '##te', '##r', '##y']\n",
      "['g', '##a', '##te', '##k', '##e', '##e', '##p', '##e', '##r']\n"
     ]
    }
   ],
   "source": [
    "T = WordpieceTokenizer()\n",
    "data = ['angel', 'of', 'vitality', 'cemetery', 'gatekeeper']\n",
    "T.train(data, 37)\n",
    "for w in data:\n",
    "    print(T.tokenize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ddw\\Anaconda3\\envs\\seq2seq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset.mtgcards import CardName\n",
    "from torchtext.legacy.data import Field\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 17014\n",
      "Number of valid data: 447\n",
      "Number of test data: 449\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_zh = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenizer_zh(text):\n",
    "    return [tok.text for tok in spacy_zh.tokenizer(text)]\n",
    "\n",
    "\n",
    "SRC = Field(tokenize = tokenizer_en, \n",
    "                init_token = '<sos>', \n",
    "                eos_token = '<eos>', \n",
    "                lower = True)\n",
    "TRG = Field(tokenize = tokenizer_zh, \n",
    "                init_token = '<sos>', \n",
    "                eos_token = '<eos>', \n",
    "                lower = True)\n",
    "fields = {'src': ('src', SRC), 'trg': ('trg', TRG)}\n",
    "train_data, valid_data, test_data = CardName.splits(fields=fields)\n",
    "\n",
    "print(f'Number of train data: {len(train_data)}')\n",
    "print(f'Number of valid data: {len(valid_data)}')\n",
    "print(f'Number of test data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11945 ['cut', 'zellix', 'nettlevine', 'bandar', 'lethemancer', 'silumgar', 'kargan', 'gnawing', 'flutterfox', 'blindblast']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for example in train_data:\n",
    "    for _ in example.src:\n",
    "        data.append(_)\n",
    "data = list(set(data))\n",
    "print(len(data), data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##g', '##e') 206\n",
      "100\n",
      "('##b', '##er') 57\n",
      "200\n",
      "('##an', '##ce') 31\n",
      "300\n",
      "('s', '##ur') 21\n",
      "400\n",
      "('c', '##al') 17\n",
      "500\n",
      "('##o', '##le') 13\n",
      "600\n",
      "('##in', '##er') 11\n",
      "700\n",
      "('ar', '##c') 9\n",
      "800\n",
      "('o', '##ff') 8\n",
      "900\n",
      "1000\n",
      "<unk> i ##r ##i ##d ##a ##n w ##g ##e ##s ##l ##b ##c ##k t ##t c ##o ##m ##p b h l ##u g s ##y e ##z f r k ##h d a q p m o n x v ##x ##v ##f u ##' ##w ##q j y z ##j ' ##. & ##- ##â ##û , ! ##é ##ü - ##ö ##er ##in ##ar ##on ##or ##an ##at ##en ##st ##al ##ing ##re ##le ##ra ##it ##ed ##ro ##el ##ri ##ou ##es ##is ##ch ##ion ##la ##il ##ic ##et ##ol ##oo ##un ##ad ##ur ##ge ##ter ##sh ##id ##ul st ##ig ##ent ##ak ##ec ##th ##om ##am ##der ##ver ##ow ##us in ##ist ##em ##ir ##ac sh ##ut sp ##um ##ss re ##ers ##ht ##im ##ve ##ag ##ap ##ast th sk ##ce ##ant ##ot ch de ##as ##ood ##ation ##ard sc ##ath ##if ##all ##ous bl un con ar ##and ##ker ##ind ##os ##ate ##per ##ight ##ct ##ire ##av ##de ##ear ##orn ##ell ##age ##op ##ru ##ck ##ble ##orm ##ine ##od ##aw ##og en ##ling ##ph ##se ##ith ex ##est ##sp ##oun ##ip ##ator ##ian ##ron ##aker ##qu ##ran ##ity ##iv ##ish ##end ##sc ##ber ##ain ##iz dis an ##ef cl pro ##aster ##ive ##ort ##ide ##ang ##ia ##ry ##ler se ##ure ##ite blood ##ay sky ##ess ##ke ##one ##tle ##bla ##az ##ack ##ly ##cer ##ark ##eth im ##bre ##row ##ff ##pt ##old ##ect ##ger ##lam ##sm ##our war ##ore tra ##oth ##ull ##oul ##ug qu ##pl sw he gra ##ab em ##ock com ##ord al ##eg ##less ##atch ##ust dra ##ious ##ring for ##bl el ##ud ##ill ##ment fl ##ash ##oot ad co wh ##iel ##ound ##rid ##oon gl dre sun bat ##pp car ##ild ##ial ##ner ##oc ##ub ##blade ##ok ##wing mar ##ance sl ##ary har ##ence ##gu ##ik ##we ##ens mind ##ll ##rig ##ai ##esh ##ancer par ne ##be fire ##inder ##rom ##are ##ult ##eart gh ma ##able sil we ##ition ##ane le be arch sn gre ##ier ##ie ##led pri ##ob pre ##eel ph ##aver ##ox ##wal ##ost ##ift ##raf ##ater pl ##ick ##mer bla ##form ##ield or ##gon am ##cla imp ##vel tre bar nec man ##ction ab ##mage ##oil ##ale ##ail ##ful ##rin ##ari ##oom ##own ##oll battle ##yn sm ##eper ##breaker ##ating ##usk ##ade ##ep ##amp ##af ##let ##ise me ##ated death ##awn des ##out ##sk te sur ##ix ##aller ##ym cra ##iss cont ##ov soul ##ax ##itch tri rec gu ##ush ##und res ##ren ob ##act ##uri ##itter my ##under ##oud mag ##ower ##atter ##fire ##ink ##erm ##ave ##arch spell ##born ##rop ##ah ##ven ##ors ##oss ##lect ##ize night flam ##lay def ##ey ##ib su ##ek ##ass ##ether ##ile ##ake ##ach ##horn ##ies ##ice ##iver bra ##arm cloud mon ##wh her inv ##wood ##master at ##adow dragon skull ##uk fro ##ign tw ##back ##art ac storm ##ris int ##alt thunder ##oy hy ##ara ##ros py ##urm run ##read ##aper fle ##rap ##right ##ome ##utter ##stone ##wor cal ##ness out ##ict ##red ser ##ward ##ions ##ringer ##caller tran ##cy over ##yr wood ##over ##ong ref ##ool silver ##smith ##eker ##walker arm ##sher ##ze wind ##atic ##dle ##ering ##ree ##oug ##ender ##kin inf ##rum ##guard mist ##ory ##ird fe ##ting kn ##mon ##omb ##ins gri cor ##bringer exp per ##enn sla ##rif dec ban ##urg ##heart ##cl ##rak ##oid ##aur tor ##aven ind dep ##ald moon ##ront ter conf ##claw bro ##land ret ##ace ##br ##raft cre dread ##keeper gro ##atcher gold dawn du comp ##ents ##ime ##oph ill ##ress ##con ##cher ##ical rep ##ray ##app ur dem ##ole aether gla gn ##ars bon ##ric wor bri ston gar ##uff ##aun bur ##te ##bon fer adv raz ##cle frost ##ook min ##air ##zz ##int cons hor ##ract mal ag ##olf ##ings ##als bre tru ##esc ##ream blight ##ere inc ##ify ##iff ##ana ##ld cap ##ather disp tro shadow ##ged light root mor ##bound ##ede ##work ##oor lif ##me under ##ister ##ann is ent heart wild ##ning ##inger coun ##lash ##ium lav ##ma ##bra ##ans om ##ider cle dream on ##for ##ex ste ##fl rot iron ##he ##olt mer ##ira ##ura ##run comm ##igh ##uck ##stru ##ster ##maw ash ##iner cry ##esp ##eaker ##hide ##cutter met ##inter thorn kar ro ##ape ##way ##gn ##rider ru chain ##weaver ##ination stone ever ##caster dom twin sch ##break col ak squ ##ought ##gen bal ##isher ##ons ##rit pe ##lord ven ##icker ##ager spl ##mark ##rown dev ##ooth ra pil ##fer ##anc flesh water ember ##ask ##elt scrap ##ied ##lame steel ##ret mir bi shield mut ##ative sand av pu ##kn ##fall ge ##ement cy necro ##ging ##ander ##ser bell ##ution ##aler rev ##plic pla ##rel sav rh ##sw dusk art ##ute lo ##na ##orph star ##amb ##unter ext ##immer ##sy cla san arc rid lum ##alv sol ara rem thought ##rush ##ys ##eck ren ##alist ##yth ghost end ass ##ible hel inter ##ief ##storm pet ##ose gen ##yre ##olog ##ue ##ther ##ridge pal ##ux ##ew reb diss ##iri ##du ##au ##flame ##oof ##allow far ##gra ver ##tain ##ogg ##aken chron ##asm ce ##usion ##inner cer ##emon amb del ##utt ev dark ham rat ##fist ##its ##craft god bo ##ste ##ich ##ev ##ted cand char ##mane ##inate ##ilt ##umb ##ader ##alk cro wolf whis ##com ##eb ##les mort wre bu go ink ##wurm ##her ##ector ##ault ##itan ##chan ##urs void ##etic cliff ##cc off ##omancer ##blast world ##ral ##urk spir ##ense ##light ##eller ##iend ##ession ##zer tal scar ##wl sy vis ##blood ##ury ##shaper ##yard ##irl sub neg ##iter dist ##warden wing dw ##rous ##ption ##uring ed ##wler path ap ##eer ##ourn tem bel br doom jun ##oke imm emp ##ters ##rage ##agger great real app ##io ##adi ##ame ##bind ##aze ##bow ##aring ##ants ##cru ##gg all ##itor tim ##ori ##ulk na aer trans ##urn gear ##layer rift ##ats med ##ays hex ##bear ##ram pain ##da ##speaker ins guild ##ship cinder ##hunter ty up fal elder ##steel rag psy psych spring fin as blade\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "T.train(random.sample(data, 10000), vocab_size=1000)\n",
    "print(len(T.vocab))\n",
    "print(*T.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "waste ['w', '##ast', '##e']\n",
      "rabblemaster ['ra', '##b', '##ble', '##master']\n",
      "akoum ['ak', '##ou', '##m']\n",
      "meditation ['med', '##it', '##ation']\n",
      "heirloom ['he', '##irl', '##oom']\n",
      "scrounging ['sc', '##ro', '##un', '##ging']\n",
      "miasma ['m', '##ia', '##sm', '##a']\n",
      "blightning ['blight', '##ning']\n",
      "dispeller ['disp', '##eller']\n",
      "syrix ['sy', '##ri', '##x']\n"
     ]
    }
   ],
   "source": [
    "# T.load('result/BPE-10k-10k.json')\n",
    "# T.vocab = [s for s in T.vocab if len(s) <= 5 or (s[0] == '#' and len(s) <= 10)]\n",
    "print(len(T.vocab))\n",
    "for word in random.sample(data, 10):\n",
    "    print(word, T.tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result/BPE-10k-1k.json', 'w') as f:\n",
    "    json.dump(list(T.vocab), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
