{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "from dataset.mtgcards import RuleText\n",
    "from utils.preprocess import fields_for_rule_text\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TRG = fields_for_rule_text()\n",
    "fields = {'src': ('src', SRC), 'trg': ('trg', TRG)}\n",
    "\n",
    "train_data, valid_data, test_data = RuleText.splits(fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (en) vocabulary: 4991\n",
      "Unique tokens in target (zh) vocabulary: 2357\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (zh) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 32]\n",
      "\t[.src]:('[torch.cuda.LongTensor of size 23x32 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
      "\t[.trg]:[torch.cuda.LongTensor of size 31x32 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device = device)\n",
    "\n",
    "tmp = next(iter(train_iterator))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model4.definition import Encoder, Attention, Decoder, Seq2Seq\n",
    "from models.model4.train import init_weights, train, evaluate\n",
    "from utils import count_parameters, train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,540,469 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model will be saved to result/tut4-model.pt\n",
      "load model parameters from result/tut4-model.pt\n",
      "Val. Loss: 1.269 |  Val. PPL:   3.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1219/1219 [01:26<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 27s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.530\n",
      "\t Val. Loss: 1.522 |  Val. PPL:   4.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 266/1219 [00:18<01:07, 14.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m TRG_PAD_IDX \u001b[39m=\u001b[39m TRG\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[TRG\u001b[39m.\u001b[39mpad_token]\n\u001b[0;32m      3\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index \u001b[39m=\u001b[39m TRG_PAD_IDX)\n\u001b[1;32m----> 5\u001b[0m train_loop(model, optimizer, criterion, train, evaluate,\n\u001b[0;32m      6\u001b[0m            train_iterator, valid_iterator, \n\u001b[0;32m      7\u001b[0m            save_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mresult/\u001b[39;49m\u001b[39m'\u001b[39;49m, file_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtut4-model.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, load_before_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\86138\\Desktop\\workspace\\utils\\__init__.py:39\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, optimizer, criterion, train, evaluate, train_iterator, valid_iterator, N_EPOCHS, CLIP, save_path, file_name, load_before_train)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m     37\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 39\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion, CLIP)\n\u001b[0;32m     40\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n\u001b[0;32m     42\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\86138\\Desktop\\workspace\\models\\model4\\train.py:46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     42\u001b[0m trg \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mtrg\n\u001b[0;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m output \u001b[39m=\u001b[39m model(src, src_len, trg)\n\u001b[0;32m     48\u001b[0m \u001b[39m#trg = [trg len, batch size]\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\86138\\Desktop\\workspace\\models\\model4\\definition.py:220\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m#mask = [batch size, src len]\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[0;32m    216\u001b[0m     \n\u001b[0;32m    217\u001b[0m     \u001b[39m#insert input token embedding, previous hidden state, all encoder hidden states \u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m#  and mask\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m#receive output tensor (predictions) and new hidden state\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     output, hidden, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39minput\u001b[39;49m, hidden, encoder_outputs, mask)\n\u001b[0;32m    222\u001b[0m     \u001b[39m#place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\86138\\Desktop\\workspace\\models\\model4\\definition.py:130\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[0;32m    126\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m))\n\u001b[0;32m    128\u001b[0m \u001b[39m#embedded = [1, batch size, emb dim]\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden, encoder_outputs, mask)\n\u001b[0;32m    132\u001b[0m \u001b[39m#a = [batch size, src len]\u001b[39;00m\n\u001b[0;32m    134\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\86138\\Desktop\\workspace\\models\\model4\\definition.py:87\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[0;32m     82\u001b[0m encoder_outputs \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[39m#hidden = [batch size, src len, dec hid dim]\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m#encoder_outputs = [batch size, src len, enc hid dim * 2]\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(torch\u001b[39m.\u001b[39;49mcat((hidden, encoder_outputs), dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m))) \n\u001b[0;32m     89\u001b[0m \u001b[39m#energy = [batch size, src len, dec hid dim]\u001b[39;00m\n\u001b[0;32m     91\u001b[0m attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv(energy)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\nn\\functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "train_loop(model, optimizer, criterion, train, evaluate,\n",
    "           train_iterator, valid_iterator, \n",
    "           save_path='result/', file_name='tut4-model.pt', load_before_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translate import Translator\n",
    "from models.model4.definition import beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('result/tut4-model.pt', map_location=torch.device(device)))\n",
    "T = Translator(SRC, TRG, model, device, beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cn>', '的', '力量', '和', '防御力', '各', '等', '同于', '以', '它', '放逐', '的', '牌', '数量', '。', '<eos>']\n",
      "['<cn>', '的', '力量', '和', '防御力', '各', '等', '同于', '它', '放逐', '之', '牌', '的', '数量', '。', '<eos>']\n",
      "['<cn>', '的', '力量', '和', '防御力', '各', '等', '同于', '以', '它', '放逐', '之', '牌', '的', '数量', '。', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "ret = T.translate('Unlicensed Hearse’s power and toughness are each equal to the number of cards exiled with it.')\n",
    "print(*ret[0][:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 45\n",
      "src: [whenever you cast an enchantment spell , if you do n't control a creature named keimi , create keimi , a legendary 3 / 3 black and green frog creature token with \" whenever you cast an enchantment spell , each opponent loses 1 life and you gain 1 life . \" ] trg = [每当你施放结界咒语时，若你未操控名称为哇魅的生物，则派出传奇衍生生物哇魅，其为3/3，黑绿双色的蛙，且具有\"每当你施放结界咒语时，每位对手各失去1点生命且你获得1点生命。\"]\n",
      "每当你施放结界咒语时，若你未操控名称为哇魅的生物哇魅，则派出传奇衍生生物哇魅，，传奇为3/3黑色，黑绿双色，且具有\"每当你施放神器，且每位对手各 \t[probability: 0.00000]\n",
      "每当你施放结界咒语时，若你未操控名称为哇魅的生物哇魅，则派出传奇衍生生物哇魅，，传奇为3/3黑色，黑绿双色，且具有\"每当你施放一个神器，且每位对手 \t[probability: 0.00000]\n",
      "每当你施放结界咒语时，若你未操控名称为哇魅的生物哇魅，则派出传奇衍生生物哇魅，，传奇为3/3黑色，黑绿双色，且具有\"每当你施放神器生物，且每位对手 \t[probability: 0.00000]\n",
      "\n",
      "src: [{ w } { u } { b } { r } { g } , { t } : put three + 1 / + 1 counters on each myr you control . ] trg = [{w}{u}{b}{r}{g}，{t}：在每个由你操控的秘耳上各放置三个+1/+1指示物。]\n",
      "{w}{u}{b}{r}{g}，{t}：在每个由你操控的秘耳上各放置三个+1/+1指示物。<eos> \t[probability: 0.13671]\n",
      "{w}{u}{b}{r}，{t}：在每个由你操控的秘耳上各放置三个+1/+1指示物。<eos> \t[probability: 0.09985]\n",
      "{w}{u}{b}{g}{g}，{t}：在每个由你操控的秘耳上各放置三个+1/+1指示物。<eos> \t[probability: 0.04061]\n",
      "\n",
      "src: [when molten firebird dies , return it to the battlefield under its owner 's control at the beginning of the next end step and you skip your next draw step . ] trg = [当<cn>从场上置入坟墓场，在回合结束时将它在其拥有者的操控下返回场上，并且你略过你的下个抓牌步骤。]\n",
      "当<cn>死去时，在下一个结束步骤开始时，你在你的坟墓场步骤开始时，将它在你的操控下移回战场。<eos> \t[probability: 0.00004]\n",
      "当<cn>死去时，在下一个结束步骤开始时，你在你的坟墓场步骤开始时，将它在你的坟墓场移回战场。<eos> \t[probability: 0.00003]\n",
      "当<cn>死去时，在下一个结束步骤开始时，你在你的坟墓场步骤开始时，将它在你的坟墓场步骤开始。<eos> \t[probability: 0.00002]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import show_samples\n",
    "long_data = [x for x in test_data.examples if len(x.src) > 30]\n",
    "print(f'Number of samples: {len(long_data)}')\n",
    "show_samples(long_data, T, n=3, beam_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:06<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.36702387304524\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_bleu\n",
    "\n",
    "bleu = calculate_bleu(long_data, lambda x: T.translate(x, beam_size=3)[0][0])\n",
    "print(bleu*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
